{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f72f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import statistics\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm, colors, colorbar\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "mmscaler = MinMaxScaler()\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# Decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "# Dimensionality reduction\n",
    "from pyts.utils import windowed_view\n",
    "\n",
    "# Statistical tests and models\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import (ttest_ind, levene, mannwhitneyu, shapiro, mstats)\n",
    "from statannot import add_stat_annotation\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# Classifiers\n",
    "rdg = RidgeClassifier(alpha=0.5)\n",
    "mlp = MLPClassifier(random_state=1, max_iter=300, activation='relu')\n",
    "lgr = LogisticRegression(random_state=1, max_iter=500)\n",
    "DT = DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "adb = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, random_state=1)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "SGD = SGDClassifier(loss='log', random_state=1, max_iter=100, early_stopping=True,\n",
    "                    learning_rate='optimal', validation_fraction=0.2)\n",
    "rf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf_svm = SVC(kernel='rbf')\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Model selection and validation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, LeaveOneOut, StratifiedKFold, GridSearchCV, cross_val_score\n",
    ")\n",
    "\n",
    "# Shapelet learning\n",
    "from pyts.classification import LearningShapelets\n",
    "from pyts.datasets import load_gunpoint\n",
    "\n",
    "# Geometry\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Others\n",
    "import umap\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import ttest_ind\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ff414",
   "metadata": {},
   "outputs": [],
   "source": [
    "OB_BL=pd.read_csv('/home/jupy/miRNA/Acute_WL/Rajesh_miRNA_data/OB_BL_MIRNA.csv')\n",
    "df_OB_BL=OB_BL.T\n",
    "\n",
    "OB_PWL=pd.read_csv('/home/jupy/miRNA/Acute_WL/Rajesh_miRNA_data/OB_PWL_MIRNA.csv')\n",
    "df_OB_PWL=OB_PWL.T\n",
    "\n",
    "LEAN=pd.read_csv('/home/jupy/miRNA/Acute_WL/Rajesh_miRNA_data/LEAN_MIRNA.csv')\n",
    "df_LEAN=LEAN.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df):\n",
    "    # Subject IDs are in the row index, we create a list\n",
    "    subject_id_ls = df.index.tolist()[2:]    \n",
    "    # Extract feature names using the row index \"Unmaed: 1\"\n",
    "    feature_id_ls=df.loc[['Unnamed: 1']].values.flatten().tolist()\n",
    "    # Etract numerical values as the miRNA value, in shape (12 subjects * 828 features)\n",
    "    feature_array=df.values[2:,:]\n",
    "    print(\"Subject IDs:\", len(subject_id_ls))\n",
    "    print(\"Feature Names:\", len(feature_id_ls))\n",
    "    print(\"miRNA Values:\", feature_array.shape)\n",
    "\n",
    "    print(\"Subject IDs:\", (subject_id_ls))\n",
    "    print(\"Feature Names:\", (feature_id_ls))\n",
    "    print(\"miRNA Values:\", feature_array)\n",
    "    print('-----------------------------------------------')\n",
    "    return subject_id_ls, feature_id_ls, feature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8c2a8",
   "metadata": {},
   "source": [
    "**nSolver** used for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73dbe2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subject_id_ls={}\n",
    "feature_id_ls={}\n",
    "feature_array={}\n",
    "for df, class_name in zip([df_OB_BL,df_OB_PWL,df_LEAN],['OB_BL','OB_PWL','LEAN']):\n",
    "    subject_id_ls[class_name]=create_data(df)[0]\n",
    "    feature_id_ls[class_name]=create_data(df)[1]\n",
    "    feature_array[class_name]=create_data(df)[2]\n",
    "    ### |Log2| transform\n",
    "    feature_array[class_name]=np.abs(np.log2( np.array( feature_array[class_name], dtype=float) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc73c0",
   "metadata": {},
   "source": [
    "#### Feature Selection and Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dli_wfs(X, y, D=10, M=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Modified DLI-WFS with performance-validated feature removal\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples, n_features)\n",
    "    - y: Target vector (n_samples,)\n",
    "    - D: Target number of selected features (default=10)\n",
    "    - M: Maximal allowed dip runs (default=10)\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - selected_features: Indices of selected features (sorted by importance)\n",
    "    - accuracy_trace: List of validation accuracies during feature selection process\n",
    "    - num_features_trace: Corresponding number of features at each step\n",
    "    \"\"\"\n",
    "    # Initialize Random Forest for feature importance\n",
    "    rf = RandomForestClassifier(random_state=random_state)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Get features sorted by importance (descending)\n",
    "    importance_scores = rf.feature_importances_\n",
    "    sorted_features = np.argsort(importance_scores)[::-1]\n",
    "    \n",
    "    # Initialize variables\n",
    "    F_s = [sorted_features[0]]  # Start with top feature\n",
    "    cruns = 0\n",
    "    j = 1\n",
    "    n_features = len(sorted_features)\n",
    "    \n",
    "    # For tracking performance during selection\n",
    "    accuracy_trace = []\n",
    "    num_features_trace = []\n",
    "    removal_decisions = []  # Track removal decisions\n",
    "    \n",
    "    # Inner CV for evaluation (5-fold stratified)\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    while j < n_features:\n",
    "        # Evaluate current feature set performance (a_o)\n",
    "        current_rf = RandomForestClassifier(random_state=random_state)\n",
    "        scores = cross_val_score(current_rf, X[:, F_s], y, cv=inner_cv, scoring='accuracy')\n",
    "        a_o = np.mean(scores)\n",
    "        \n",
    "        # Add next feature\n",
    "        F_s.append(sorted_features[j])\n",
    "        \n",
    "        # Evaluate new performance (a_n)\n",
    "        current_rf = RandomForestClassifier(random_state=random_state)\n",
    "        scores = cross_val_score(current_rf, X[:, F_s], y, cv=inner_cv, scoring='accuracy')\n",
    "        a_n = np.mean(scores)\n",
    "        \n",
    "        # Record performance before any removal\n",
    "        accuracy_trace.append(a_n)\n",
    "        num_features_trace.append(len(F_s))\n",
    "        \n",
    "        if a_n - a_o > 0.1:\n",
    "            cruns = 0  # Reset dip counter if performance improved\n",
    "        else:\n",
    "            cruns += 1  # Increment dip counter\n",
    "            \n",
    "            # Store original state before removal attempt\n",
    "            original_features = F_s.copy()\n",
    "            original_acc = a_n\n",
    "            \n",
    "            # Compute feature importance on current subset\n",
    "            subset_rf = RandomForestClassifier(random_state=random_state)\n",
    "            subset_rf.fit(X[:, F_s], y)\n",
    "            U = subset_rf.feature_importances_\n",
    "            \n",
    "            # Find least important feature\n",
    "            f_bad = F_s[np.argmin(U)]\n",
    "            \n",
    "            # Tentatively remove the feature\n",
    "            F_s.remove(f_bad)\n",
    "            \n",
    "            # Evaluate performance after removal\n",
    "            current_rf = RandomForestClassifier(random_state=random_state)\n",
    "            scores = cross_val_score(current_rf, X[:, F_s], y, cv=inner_cv, scoring='accuracy')\n",
    "            a_after_removal = np.mean(scores)\n",
    "            \n",
    "            # Performance validation check\n",
    "            if a_after_removal > original_acc:\n",
    "                # Keep the removal - it improved performance\n",
    "                removal_decisions.append(f\"Removed {f_bad} - Improved accuracy from {original_acc:.4f} to {a_after_removal:.4f}\")\n",
    "                # Update the last accuracy point\n",
    "                accuracy_trace[-1] = a_after_removal\n",
    "                num_features_trace[-1] = len(F_s)\n",
    "            else:\n",
    "                # Revert the removal - it didn't help\n",
    "                F_s = original_features\n",
    "                removal_decisions.append(f\"Retained {f_bad} - Removal would decrease accuracy from {original_acc:.4f} to {a_after_removal:.4f}\")\n",
    "                # Keep the original accuracy measurement\n",
    "                accuracy_trace[-1] = original_acc\n",
    "                num_features_trace[-1] = len(F_s)\n",
    "        \n",
    "        # Check stopping conditions\n",
    "        if cruns == M or len(F_s) >= D:\n",
    "            break\n",
    "            \n",
    "        j += 1\n",
    "    \n",
    "    # Print removal decisions for debugging\n",
    "    print(\"\\nFeature Removal Decisions:\")\n",
    "    for decision in removal_decisions:\n",
    "        print(decision)\n",
    "    \n",
    "    return np.array(F_s), accuracy_trace, num_features_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda24772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Evaluation Metrics ======================\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def sensitivity_score(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred)  # Sensitivity is the same as recall\n",
    "\n",
    "def evaluate_with_multiple_seeds(X_train, y_train, X_test, y_test, feature_indices, n_seeds=100):\n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'specificity': [],\n",
    "        'sensitivity': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    for seed in range(n_seeds):\n",
    "        # Use only the selected features\n",
    "        X_train_sub = X_train[:, feature_indices]\n",
    "        X_test_sub = X_test[:, feature_indices]\n",
    "        \n",
    "        # Train and predict with current seed\n",
    "        rf = RandomForestClassifier(random_state=seed)\n",
    "        rf.fit(X_train_sub, y_train)\n",
    "        \n",
    "        # For AUC we need probability predictions\n",
    "        if len(np.unique(y_test)) > 1:  # Check if we have both classes in test set\n",
    "            y_probs = rf.predict_proba(X_test_sub)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_probs)\n",
    "        else:\n",
    "            auc = np.nan\n",
    "        \n",
    "        y_pred = rf.predict(X_test_sub)\n",
    "        \n",
    "        # Store all metrics\n",
    "        metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        metrics['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "        metrics['specificity'].append(specificity_score(y_test, y_pred))\n",
    "        metrics['sensitivity'].append(sensitivity_score(y_test, y_pred))\n",
    "        metrics['auc'].append(auc)\n",
    "    \n",
    "    # Return average metrics\n",
    "    return {k: np.nanmean(v) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c49da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Main Experiment Function ======================\n",
    "def run_experiment_with_dliwfs(X, y, D=10, M=10, top_k=5, n_seeds=3, random_state=42):\n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'fold_test_metrics': [],\n",
    "        'selected_features': [],\n",
    "        'plots': []\n",
    "    }\n",
    "    \n",
    "    # Outer 10-fold stratified CV\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(cv.split(X, y), total=10)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Run DLI-WFS feature selection\n",
    "        selected_features, acc_trace, num_feat_trace = dli_wfs(\n",
    "            X_train, y_train, D=D, M=M, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Store selected features (indices)\n",
    "        results['selected_features'].append(selected_features)\n",
    "        \n",
    "        # Get top k features\n",
    "        top_features = selected_features[:top_k]\n",
    "        \n",
    "        # Evaluate with multiple seeds\n",
    "        fold_metrics = evaluate_with_multiple_seeds(\n",
    "            X_train, y_train, X_test, y_test, top_features, n_seeds=n_seeds\n",
    "        )\n",
    "        results['fold_test_metrics'].append(fold_metrics)\n",
    "        \n",
    "        # Create plot for this fold\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(num_feat_trace, acc_trace, 'o-')\n",
    "        ax.set_xlabel('Number of Selected Features')\n",
    "        ax.set_ylabel('Validation Accuracy')\n",
    "        ax.set_title(f'Fold {fold+1}: Feature Selection Process\\n'\n",
    "                    f'Top {top_k} features used for testing')\n",
    "        ax.grid(True)\n",
    "        results['plots'].append(fig)\n",
    "        \n",
    "        # Print fold summary\n",
    "        print(f\"\\nFold {fold+1} completed:\")\n",
    "        print(f\"Selected features (indices): {selected_features}\")\n",
    "        print(f\"Top {top_k} features used: {top_features}\")\n",
    "        print(\"Test metrics (averaged over 100 seeds):\")\n",
    "        for metric, value in fold_metrics.items():\n",
    "            print(f\"{metric:12s}: {value:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics across all folds\n",
    "    avg_metrics = {\n",
    "        metric: np.mean([fold[metric] for fold in results['fold_test_metrics']])\n",
    "        for metric in results['fold_test_metrics'][0].keys()\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Final Average Metrics ===\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric:12s}: {value:.4f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02613e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_with_boruta(X, y, top_k=5, n_seeds=3, random_state=42):\n",
    "    \"\"\"\n",
    "    Main experiment function using Boruta for feature selection\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples, n_features)\n",
    "    - y: Target vector (n_samples,)\n",
    "    - top_k: Number of top features to select and use for final evaluation\n",
    "    - n_seeds: Number of random seeds for evaluation\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing results similar to original experiment\n",
    "    \"\"\"\n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'fold_test_metrics': [],\n",
    "        'selected_features': [],\n",
    "        'plots': []\n",
    "    }\n",
    "    \n",
    "    # Outer 10-fold stratified CV\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(cv.split(X, y), total=10)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Initialize Random Forest and Boruta\n",
    "        rf = RandomForestClassifier(n_jobs=-1, random_state=random_state)\n",
    "        boruta_selector = BorutaPy(\n",
    "            estimator=rf,\n",
    "            n_estimators='auto',\n",
    "            verbose=0,\n",
    "            random_state=random_state,\n",
    "            alpha=0.05,\n",
    "            max_iter=100,\n",
    "            perc=100)  # Setting perc=100 makes Boruta more conservative\n",
    "        \n",
    "        # Run Boruta feature selection\n",
    "        boruta_selector.fit(X_train, y_train)\n",
    "        \n",
    "        # Get importance scores from Boruta\n",
    "        feature_importances = boruta_selector.ranking_\n",
    "        \n",
    "        # Select top_k features with lowest Boruta ranks (1 is best)\n",
    "        selected_features = np.argsort(feature_importances)[:top_k]\n",
    "        \n",
    "        # Store selected features\n",
    "        results['selected_features'].append(selected_features)\n",
    "        \n",
    "        # Get importance scores for selected features by training a new RF\n",
    "        rf_selected = RandomForestClassifier(random_state=random_state)\n",
    "        rf_selected.fit(X_train[:, selected_features], y_train)\n",
    "        importances = rf_selected.feature_importances_\n",
    "        \n",
    "        # Sort the selected features by their importance\n",
    "        top_features = selected_features[np.argsort(importances)[::-1]]\n",
    "        \n",
    "        # Evaluate with multiple seeds\n",
    "        fold_metrics = evaluate_with_multiple_seeds(\n",
    "            X_train, y_train, X_test, y_test, top_features, n_seeds=n_seeds\n",
    "        )\n",
    "        results['fold_test_metrics'].append(fold_metrics)\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(range(len(importances)), importances)\n",
    "        ax.set_xlabel('Feature Index (in selected features)')\n",
    "        ax.set_ylabel('Importance Score')\n",
    "        ax.set_title(f'Fold {fold+1}: Feature Importances\\n'\n",
    "                    f'Top {top_k} features used for testing')\n",
    "        ax.set_xticks(range(len(selected_features)))\n",
    "        ax.set_xticklabels(selected_features)\n",
    "        ax.grid(True)\n",
    "        results['plots'].append(fig)\n",
    "        \n",
    "        # Print fold summary\n",
    "        print(f\"\\nFold {fold+1} completed:\")\n",
    "        print(f\"Selected features (indices): {selected_features}\")\n",
    "        print(f\"Top {top_k} features sorted by importance: {top_features}\")\n",
    "        print(f\"Test metrics (averaged over {n_seeds} seeds):\")\n",
    "        for metric, value in fold_metrics.items():\n",
    "            print(f\"{metric:12s}: {value:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics across all folds\n",
    "    avg_metrics = {\n",
    "        metric: np.mean([fold[metric] for fold in results['fold_test_metrics']])\n",
    "        for metric in results['fold_test_metrics'][0].keys()\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Final Average Metrics ===\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric:12s}: {value:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991783cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_with_rf_feature_selection(X, y, top_k=5, n_seeds=3, random_state=42):\n",
    "    \"\"\"\n",
    "    Main experiment function using Random Forest feature importance for feature selection\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples, n_features)\n",
    "    - y: Target vector (n_samples,)\n",
    "    - top_k: Number of top features to select and use for final evaluation\n",
    "    - n_seeds: Number of random seeds for evaluation\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing results similar to original experiment\n",
    "    \"\"\"\n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'fold_test_metrics': [],\n",
    "        'selected_features': [],\n",
    "        'plots': []\n",
    "    }\n",
    "    \n",
    "    # Outer 10-fold stratified CV\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(cv.split(X, y), total=10)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Initialize and train Random Forest\n",
    "        rf = RandomForestClassifier(n_jobs=-1, random_state=random_state)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        \n",
    "        # Select top_k features with highest importance\n",
    "        selected_features = np.argsort(importances)[-top_k:][::-1]\n",
    "        \n",
    "        # Store selected features\n",
    "        results['selected_features'].append(selected_features)\n",
    "        \n",
    "        # Get importance scores for selected features by training a new RF\n",
    "        # (This ensures we get importance scores just for the selected features)\n",
    "        rf_selected = RandomForestClassifier(random_state=random_state)\n",
    "        rf_selected.fit(X_train[:, selected_features], y_train)\n",
    "        selected_importances = rf_selected.feature_importances_\n",
    "        \n",
    "        # Sort the selected features by their importance\n",
    "        top_features = selected_features[np.argsort(selected_importances)[::-1]]\n",
    "        \n",
    "        # Evaluate with multiple seeds\n",
    "        fold_metrics = evaluate_with_multiple_seeds(\n",
    "            X_train, y_train, X_test, y_test, top_features, n_seeds=n_seeds\n",
    "        )\n",
    "        results['fold_test_metrics'].append(fold_metrics)\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(range(len(selected_importances)), selected_importances)\n",
    "        ax.set_xlabel('Feature Index (in selected features)')\n",
    "        ax.set_ylabel('Importance Score')\n",
    "        ax.set_title(f'Fold {fold+1}: Feature Importances\\n'\n",
    "                    f'Top {top_k} features used for testing')\n",
    "        ax.set_xticks(range(len(selected_features)))\n",
    "        ax.set_xticklabels(selected_features)\n",
    "        ax.grid(True)\n",
    "        results['plots'].append(fig)\n",
    "        \n",
    "        # Print fold summary\n",
    "        print(f\"\\nFold {fold+1} completed:\")\n",
    "        print(f\"Selected features (indices): {selected_features}\")\n",
    "        print(f\"Top {top_k} features sorted by importance: {top_features}\")\n",
    "        print(f\"Test metrics (averaged over {n_seeds} seeds):\")\n",
    "        for metric, value in fold_metrics.items():\n",
    "            print(f\"{metric:12s}: {value:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics across all folds\n",
    "    avg_metrics = {\n",
    "        metric: np.mean([fold[metric] for fold in results['fold_test_metrics']])\n",
    "        for metric in results['fold_test_metrics'][0].keys()\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Final Average Metrics ===\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric:12s}: {value:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427f9ac",
   "metadata": {},
   "source": [
    "# Set X and y (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BLvsL=np.vstack([feature_array['OB_BL'],feature_array['LEAN']])\n",
    "y_BLvsL=np.array([1]*feature_array['OB_BL'].shape[0]+[0]*feature_array['LEAN'].shape[0])\n",
    "print(X_BLvsL.shape,y_BLvsL.shape)\n",
    "\n",
    "X=X_BLvsL\n",
    "y=y_BLvsL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_results_to_txt(results, filename=\"log2_experiment_results.txt\"):\n",
    "    \"\"\"\n",
    "    Save experiment results dictionary to a text file\n",
    "    \n",
    "    Parameters:\n",
    "    - results: Dictionary containing experiment results\n",
    "    - filename: Name of the output file\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write basic metrics\n",
    "        if 'fold_test_metrics' in results:\n",
    "            f.write(\"=== Average Metrics Across Folds ===\\n\")\n",
    "            avg_metrics = {\n",
    "                metric: np.mean([fold[metric] for fold in results['fold_test_metrics']])\n",
    "                for metric in results['fold_test_metrics'][0].keys()\n",
    "            }\n",
    "            for metric, value in avg_metrics.items():\n",
    "                f.write(f\"{metric:12s}: {value:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Write selected features for each fold\n",
    "        if 'selected_features' in results:\n",
    "            f.write(\"=== Selected Features Per Fold ===\\n\")\n",
    "            for fold_idx, features in enumerate(results['selected_features']):\n",
    "                f.write(f\"Fold {fold_idx+1}: {features}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Write detailed fold metrics\n",
    "        if 'fold_test_metrics' in results:\n",
    "            f.write(\"=== Detailed Metrics Per Fold ===\\n\")\n",
    "            for fold_idx, metrics in enumerate(results['fold_test_metrics']):\n",
    "                f.write(f\"Fold {fold_idx+1}:\\n\")\n",
    "                for metric, value in metrics.items():\n",
    "                    f.write(f\"  {metric:12s}: {value:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "def save_results_to_json(results, filename=\"log2_RF_results.json\"):\n",
    "    # Convert numpy arrays to lists\n",
    "    results_serializable = {\n",
    "        'fold_test_metrics': results['fold_test_metrics'],\n",
    "        'selected_features': [arr.tolist() for arr in results['selected_features']]\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results_serializable, f, indent=4)\n",
    "        \n",
    "def load_results_from_json(filename=\"RF_results.json\"):\n",
    "    \"\"\"Load results from JSON file back into original dictionary format\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        loaded_data = json.load(f)\n",
    "    \n",
    "    # Reconstruct the original dictionary structure\n",
    "    results = {\n",
    "        'fold_test_metrics': loaded_data['fold_test_metrics'],\n",
    "        'selected_features': [np.array(features) for features in loaded_data['selected_features']],\n",
    "        'plots': []  # Plots would need to be regenerated\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42d070",
   "metadata": {},
   "source": [
    "Svae and Read from .jason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b48132-f9ce-44ce-87f1-ff80981e90bc",
   "metadata": {},
   "source": [
    "# FS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b06ea3ee-b050-41d4-b813-12c8f7a08c2b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "os.getcwd()\n",
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')\n",
    "\n",
    "# ====================== Example Usage BuROTA======================\n",
    "if __name__ == \"__main__\":    \n",
    "    boruta_default_results = run_experiment_with_boruta(X, y, top_k=5, n_seeds=100, random_state=0)\n",
    "    \n",
    "# ====================== Example Usage DLI-WFS ======================\n",
    "if __name__ == \"__main__\":  \n",
    "    dliwfs_default_results= run_experiment_with_dliwfs(X, y, D=5, M=30, top_k=5, n_seeds=100, random_state=42)\n",
    "        \n",
    "# ====================== Simple RF ======================\n",
    "if __name__ == \"__main__\":\n",
    "    simpleRF_default_results= run_experiment_with_rf_feature_selection(X, y, top_k=5, n_seeds=100, random_state=42)\n",
    "\n",
    "\n",
    "save_results_to_txt(boruta_default_results, \"boruta_default_results.txt\")\n",
    "save_results_to_txt(dliwfs_default_results, \"dliwfs_default_results.txt\")\n",
    "save_results_to_txt(simpleRF_default_results, \"simpleRF_default_results.txt\")\n",
    "\n",
    "save_results_to_json(boruta_default_results, \"boruta_default_results.json\")\n",
    "save_results_to_json(dliwfs_default_results, \"dliwfs_default_results.json\")\n",
    "save_results_to_json(simpleRF_default_results, \"simpleRF_default_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')\n",
    "simpleRF_default_results=load_results_from_json(\"simpleRF_default_results.jason\")\n",
    "dliwfs_default_results=load_results_from_json(\"dliwfs_default_results.jason\")\n",
    "boruta_default_results=load_results_from_json(\"boruta_default_results.jason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')\n",
    "\n",
    "from collections import defaultdict\n",
    "# Prepare data\n",
    "methods = {\n",
    "    'DLIWFS': dliwfs_default_results['selected_features'],\n",
    "    'Boruta': boruta_default_results['selected_features'],\n",
    "    'SimpleRF': simpleRF_default_results['selected_features']\n",
    "}\n",
    "\n",
    "# Get all unique features across all methods\n",
    "all_features = set()\n",
    "for method_features in methods.values():\n",
    "    for arr in method_features:\n",
    "        all_features.update(arr)\n",
    "all_features = sorted(all_features)\n",
    "\n",
    "# Create count matrix: rows=features, columns=methods\n",
    "count_matrix = np.zeros((len(all_features), len(methods)))\n",
    "for i, feature in enumerate(all_features):\n",
    "    for j, (method_name, method_features) in enumerate(methods.items()):\n",
    "        count = sum(feature in arr for arr in method_features)\n",
    "        count_matrix[i, j] = count\n",
    "\n",
    "# Sort features by total count (sum across methods)\n",
    "total_counts = count_matrix.sum(axis=1)\n",
    "sort_idx = np.argsort(-total_counts)  # Descending order\n",
    "sorted_features = np.array(all_features)[sort_idx]\n",
    "sorted_counts = count_matrix[sort_idx]\n",
    "\n",
    "sorted_features_BL=sorted_features\n",
    "\n",
    "# Get feature names\n",
    "feature_names = np.array(feature_id_ls['OB_BL'])[sorted_features]\n",
    "\n",
    "# Create stacked bar plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Distinct colors for each method\n",
    "bar_width = 0.8\n",
    "\n",
    "# Plot stacked bars\n",
    "bottom = np.zeros(len(sorted_features))\n",
    "for j, method_name in enumerate(methods.keys()):\n",
    "    plt.bar(range(len(sorted_features)), sorted_counts[:, j], \n",
    "            bottom=bottom, color=colors[j], \n",
    "            width=bar_width, label=method_name)\n",
    "    bottom += sorted_counts[:, j]\n",
    "\n",
    "# Customize plot\n",
    "plt.title('MicroRNA Feature Selection Frequency by Method (Baseline Comparison)')\n",
    "plt.xlabel('MicroRNA Features')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.xticks(range(len(sorted_features)), feature_names, rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.legend(title='Feature Selection Method')\n",
    "\n",
    "# Add value labels for total counts\n",
    "for i, total in enumerate(bottom):\n",
    "    if total > 0:  # Only label bars with some counts\n",
    "        plt.text(i, total + 0.2, f'{int(total)}', \n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_selection_feature_frequency_comparison_BL.pdf', \n",
    "            format='pdf', \n",
    "            bbox_inches='tight', \n",
    "            dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print sorted frequency table\n",
    "print(\"Feature Frequency by Method (sorted by total frequency):\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Feature\\t\\tTotal\\tDLIWFS\\tBoruta\\tSimpleRF\")\n",
    "for i, feature in enumerate(sorted_features):\n",
    "    counts = sorted_counts[i]\n",
    "    if sum(counts) > 0:  # Only print features that were selected\n",
    "        print(f\"{feature_names[i]}\\t{int(sum(counts))}\\t{int(counts[0])}\\t{int(counts[1])}\\t{int(counts[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20789b56",
   "metadata": {},
   "source": [
    "### Why limit our scope to 7 top features \n",
    "Here we take the final 7 as features within scope, because 1) suggested by other research usually 3-7 features shows strong differentiation power in microRNA classfication; 2) the top 7 features are selected by at least 2 algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3693c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_analysed_BL=7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3a61f",
   "metadata": {},
   "source": [
    "# Set X and y (PWL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PWLvsL=np.vstack([feature_array['OB_PWL'],feature_array['LEAN']])\n",
    "y_PWLvsL=np.array([1]*feature_array['OB_PWL'].shape[0]+[0]*feature_array['LEAN'].shape[0])\n",
    "print(X_PWLvsL.shape,y_PWLvsL.shape)\n",
    "\n",
    "X=X_PWLvsL\n",
    "y=y_PWLvsL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5859d-c4df-495f-b238-f01014d5b0b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')\n",
    "\n",
    "# ====================== Example Usage BuROTA======================\n",
    "if __name__ == \"__main__\":    \n",
    "    boruta_default_results_PWL = run_experiment_with_boruta(X, y, top_k=5, n_seeds=100, random_state=0)\n",
    "    \n",
    "# ====================== Example Usage DLI-WFS ======================\n",
    "if __name__ == \"__main__\":  \n",
    "    dliwfs_default_results_PWL = run_experiment_with_dliwfs(X, y, D=5, M=30, top_k=5, n_seeds=100, random_state=42)\n",
    "        \n",
    "# ====================== Simple RF ======================\n",
    "if __name__ == \"__main__\":\n",
    "    simpleRF_default_results_PWL = run_experiment_with_rf_feature_selection(X, y, top_k=5, n_seeds=100, random_state=42)\n",
    "\n",
    "save_results_to_txt(boruta_default_results_PWL, \"boruta_default_results_PWL.txt\")\n",
    "save_results_to_txt(dliwfs_default_results_PWL, \"dliwfs_default_results_PWL.txt\")\n",
    "save_results_to_txt(simpleRF_default_results_PWL, \"simpleRF_default_results_PWL.txt\")\n",
    "\n",
    "save_results_to_json(boruta_default_results_PWL, \"boruta_default_results_PWL.json\")\n",
    "save_results_to_json(dliwfs_default_results_PWL, \"dliwfs_default_results_PWL.json\")\n",
    "save_results_to_json(simpleRF_default_results_PWL, \"simpleRF_default_results_PWL.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e73572",
   "metadata": {},
   "source": [
    "Load Results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33eb629f-8b64-45d7-8ed0-4c39ce6641db",
   "metadata": {},
   "source": [
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')\n",
    "simpleRF_default_results_PWL=load_results_from_json(\"simpleRF_default_results_PWL.json\")\n",
    "dliwfs_default_results_PWL=load_results_from_json(\"dliwfs_default_results_PWL.json\")\n",
    "boruta_default_results_PWL=load_results_from_json(\"boruta_default_results_PWL.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')\n",
    "\n",
    "methods = {\n",
    "    'DLIWFS': dliwfs_default_results_PWL['selected_features'],\n",
    "    'Boruta': boruta_default_results_PWL['selected_features'],\n",
    "    'SimpleRF': simpleRF_default_results_PWL['selected_features']\n",
    "}\n",
    "\n",
    "# Get all unique features across all methods\n",
    "all_features = set()\n",
    "for method_features in methods.values():\n",
    "    for arr in method_features:\n",
    "        all_features.update(arr)\n",
    "all_features = sorted(all_features)\n",
    "\n",
    "# Create count matrix: rows=features, columns=methods\n",
    "count_matrix = np.zeros((len(all_features), len(methods)))\n",
    "for i, feature in enumerate(all_features):\n",
    "    for j, (method_name, method_features) in enumerate(methods.items()):\n",
    "        count = sum(feature in arr for arr in method_features)\n",
    "        count_matrix[i, j] = count\n",
    "\n",
    "# Sort features by total count (sum across methods)\n",
    "total_counts = count_matrix.sum(axis=1)\n",
    "sort_idx = np.argsort(-total_counts)  # Descending order\n",
    "sorted_features = np.array(all_features)[sort_idx]\n",
    "sorted_counts = count_matrix[sort_idx]\n",
    "\n",
    "sorted_features_PWL=sorted_features\n",
    "\n",
    "# Get feature names\n",
    "feature_names = np.array(feature_id_ls['OB_BL'])[sorted_features]\n",
    "\n",
    "# Create stacked bar plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Distinct colors for each method\n",
    "bar_width = 0.8\n",
    "\n",
    "# Plot stacked bars\n",
    "bottom = np.zeros(len(sorted_features))\n",
    "for j, method_name in enumerate(methods.keys()):\n",
    "    plt.bar(range(len(sorted_features)), sorted_counts[:, j], \n",
    "            bottom=bottom, color=colors[j], \n",
    "            width=bar_width, label=method_name)\n",
    "    bottom += sorted_counts[:, j]\n",
    "\n",
    "# Customize plot\n",
    "plt.title('MicroRNA Feature Selection Frequency by Method (Post Weight-Loss Comparison)')\n",
    "plt.xlabel('MicroRNA Features')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.xticks(range(len(sorted_features)), feature_names, rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.legend(title='Feature Selection Method')\n",
    "\n",
    "# Add value labels for total counts\n",
    "for i, total in enumerate(bottom):\n",
    "    if total > 0:  # Only label bars with some counts\n",
    "        plt.text(i, total + 0.2, f'{int(total)}', \n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_selection_feature_frequency_comparison_PWL.pdf', \n",
    "            format='pdf', \n",
    "            bbox_inches='tight', \n",
    "            dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print sorted frequency table\n",
    "print(\"Feature Frequency by Method (sorted by total frequency):\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Feature\\t\\tTotal\\tDLIWFS\\tBoruta\\tSimpleRF\")\n",
    "for i, feature in enumerate(sorted_features):\n",
    "    counts = sorted_counts[i]\n",
    "    if sum(counts) > 0:  # Only print features that were selected\n",
    "        print(f\"{feature_names[i]}\\t{int(sum(counts))}\\t{int(counts[0])}\\t{int(counts[1])}\\t{int(counts[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6639c",
   "metadata": {},
   "source": [
    "#### limit our scope to 6 features here \n",
    "Use the similar jutisfication, we limit our scope of features to analyse to 6. Becuase 6 is between 3-8. And the top 6 features are selected by at least two agorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_analysed_PWL=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e1bd1e",
   "metadata": {},
   "source": [
    "# Obese Baseline vs Post Weigt-Loss Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_OB_PWLvsX_OB_BL=np.vstack([feature_array['OB_PWL'],feature_array['OB_BL']])\n",
    "y_OB_PWLvsy_OB_BL=np.array([1]*feature_array['OB_PWL'].shape[0]+[0]*feature_array['OB_BL'].shape[0])\n",
    "\n",
    "X=X_OB_PWLvsX_OB_BL\n",
    "y=y_OB_PWLvsy_OB_BL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71ab810a",
   "metadata": {},
   "source": [
    "# ====================== Example Usage BuROTA======================\n",
    "if __name__ == \"__main__\":    \n",
    "    boruta_default_results_OB_BL = run_experiment_with_boruta(X, y, top_k=5, n_seeds=100, random_state=0)\n",
    "    \n",
    "# ====================== Example Usage DLI-WFS ======================\n",
    "if __name__ == \"__main__\":  \n",
    "    dliwfs_default_results_OB_BL  = run_experiment_with_dliwfs(X, y, D=5, M=30, top_k=5, n_seeds=100, random_state=42)\n",
    "        \n",
    "# ====================== Simple RF ======================\n",
    "if __name__ == \"__main__\":\n",
    "    simpleRF_default_results_OB_BL  = run_experiment_with_rf_feature_selection(X, y, top_k=5, n_seeds=100, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22d07a8e",
   "metadata": {},
   "source": [
    "os.getcwd()\n",
    "os.chdir('/home/jupy/miRNA/Results')\n",
    "\n",
    "save_results_to_txt(boruta_default_results_OB_BL, \"boruta_default_results_OB_BL.txt\")\n",
    "save_results_to_txt(dliwfs_default_results_OB_BL, \"dliwfs_default_results_OB_BL.txt\")\n",
    "save_results_to_txt(simpleRF_default_results_OB_BL, \"simpleRF_default_results_OB_BL.txt\")\n",
    "\n",
    "save_results_to_json(boruta_default_results_OB_BL, \"boruta_default_results_OB_BL.json\")\n",
    "save_results_to_json(dliwfs_default_results_OB_BL, \"dliwfs_default_results_OB_BL.json\")\n",
    "save_results_to_json(simpleRF_default_results_OB_BL, \"simpleRF_default_results_OB_BL.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca473f57",
   "metadata": {},
   "source": [
    "The classification performance is at chance level. This makes sense as our small sample size is not enough for the ML to capture complex/intricate differnce betweeen pre and post weight-loss within the obese cohort. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813005c",
   "metadata": {},
   "source": [
    "# Observe Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e282530",
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_features_to_analyse=sorted_features_BL[0:n_features_analysed_BL]\n",
    "PWL_features_to_analyse=sorted_features_PWL[0:n_features_analysed_PWL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f83d5-c3aa-429e-8803-5d8b1912dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sets\n",
    "set_bl = set(np.array(feature_id_ls['OB_BL'])[BL_features_to_analyse])\n",
    "set_pwl = set(np.array(feature_id_ls['OB_BL'])[PWL_features_to_analyse])\n",
    "\n",
    "# Create Venn diagram\n",
    "venn = venn2([set_bl, set_pwl], set_labels=('L vs OB_BL', 'L vs OB_PWL'))\n",
    "\n",
    "# Customize labels to show actual elements\n",
    "venn.get_label_by_id('10').set_text('\\n'.join(set_bl - set_pwl))      # Only in BL\n",
    "venn.get_label_by_id('01').set_text('\\n'.join(set_pwl - set_bl))      # Only in PWL\n",
    "venn.get_label_by_id('11').set_text('\\n'.join(set_bl & set_pwl))      # In both\n",
    "\n",
    "# Display plot\n",
    "plt.title(\"Venn Diagram of Feature Overlap Between Different Classification Tasks\")\n",
    "#plt.savefig('venn_feature_overlap.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b264d51",
   "metadata": {},
   "source": [
    "Use tsne to visualisation high dimensional feature space:\n",
    "<br> Hypothesis 1: Features responde to WL (only in BL) should make Lean and PWL closer and BL far from these two\n",
    "<br> Hypothesis 2: Features persist to WL (both BL and PWL) should make BL and PWL closer and Lean far from these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WL_respond_features=list(set(BL_features_to_analyse)-set(PWL_features_to_analyse))#list(set_bl - set_pwl)\n",
    "WL_persist_features=list(set(BL_features_to_analyse)&set(PWL_features_to_analyse))#list(set_bl & set_pwl)\n",
    "WL_remodel_features=list(set(PWL_features_to_analyse)-set(BL_features_to_analyse))\n",
    "\n",
    "WL_respond_feature_names = np.array(feature_id_ls['OB_BL'])[WL_respond_features]\n",
    "WL_persist_feature_names = np.array(feature_id_ls['OB_BL'])[WL_persist_features]\n",
    "WL_remodel_feature_names=np.array(feature_id_ls['OB_BL'])[WL_remodel_features]\n",
    "\n",
    "# Create DataFrames\n",
    "df_WL_respond_features= pd.DataFrame({\n",
    "    'feature_id': WL_respond_features,\n",
    "    'feature_name': WL_respond_feature_names })\n",
    "df_WL_persist_features = pd.DataFrame({\n",
    "    'feature_id': WL_persist_features,\n",
    "    'feature_name': WL_persist_feature_names})\n",
    "df_WL_remodel_features = pd.DataFrame({\n",
    "    'feature_id': WL_remodel_features,\n",
    "    'feature_name': WL_remodel_feature_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147bce10",
   "metadata": {},
   "source": [
    "#### Try to classify using only WL-persistent features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_cv_performance(X, y, n_estimators=100, random_state=None):\n",
    "    \"\"\"\n",
    "    Evaluate Random Forest classifier performance using 10-fold stratified cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples, n_features)\n",
    "    - y: Target vector (n_samples,)\n",
    "    - n_estimators: Number of trees in the forest\n",
    "    - random_state: Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary containing lists of performance metrics across all folds:\n",
    "    'accuracy', 'precision', 'recall', 'f1', 'specificity', 'sensitivity', 'auc'\n",
    "    \"\"\"\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    \n",
    "    # Initialize stratified k-fold\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'specificity': [],\n",
    "        'sensitivity': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train the model\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = rf.predict(X_test)\n",
    "        y_prob = rf.predict_proba(X_test)[:, 1]  # probabilities for positive class\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        metrics['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        # Calculate specificity and sensitivity from confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # same as recall\n",
    "        \n",
    "        metrics['specificity'].append(specificity)\n",
    "        metrics['sensitivity'].append(sensitivity)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "        except ValueError:\n",
    "            # Handle case when only one class is present\n",
    "            auc = 0.5\n",
    "        metrics['auc'].append(auc)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_performance_metrics(performance_metrics):\n",
    "    \n",
    "    # Calculate mean of each metric\n",
    "    mean_metrics = {\n",
    "        'accuracy': np.mean(performance_metrics['accuracy']),\n",
    "        'precision': np.mean(performance_metrics['precision']),\n",
    "        'recall': np.mean(performance_metrics['recall']),\n",
    "        'f1': np.mean(performance_metrics['f1']),\n",
    "        'specificity': np.mean(performance_metrics['specificity']),\n",
    "        'sensitivity': np.mean(performance_metrics['sensitivity']),\n",
    "        'auc': np.mean(performance_metrics['auc'])\n",
    "    }\n",
    "\n",
    "    # Print the average performance metrics\n",
    "    print(\"\\nAverage Performance Metrics Across Folds:\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"Accuracy:    {mean_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision:   {mean_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:      {mean_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:    {mean_metrics['f1']:.4f}\")\n",
    "    print(f\"Specificity: {mean_metrics['specificity']:.4f}\")\n",
    "    print(f\"Sensitivity: {mean_metrics['sensitivity']:.4f}\")\n",
    "    print(f\"AUC:         {mean_metrics['auc']:.4f}\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    # Calculate and print standard deviations\n",
    "    std_metrics = {\n",
    "        metric: np.std(values) \n",
    "        for metric, values in performance_metrics.items()\n",
    "    }\n",
    "\n",
    "    print(\"\\nStandard Deviations Across Folds:\")\n",
    "    print(\"--------------------------------\")\n",
    "    for metric, std in std_metrics.items():\n",
    "        print(f\"{metric.capitalize():<12}: {std:.4f}\")\n",
    "    print(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07497a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def run_multiple_seeds(X, y, n_estimators=100, num_seeds=100, output_file=\"rf_multiseed_results.txt\"):\n",
    "    \"\"\"\n",
    "    Run random forest CV performance with multiple seeds and save results.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - y: Target vector\n",
    "    - n_estimators: Number of trees in random forest\n",
    "    - num_seeds: Number of different random seeds to try\n",
    "    - output_file: Name of the output text file\n",
    "    \"\"\"\n",
    "    # Initialize storage for all results\n",
    "    all_results = {\n",
    "        'seed': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'specificity': [],\n",
    "        'sensitivity': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    # Create header for output file\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    header = f\"Random Forest Performance Across {num_seeds} Seeds\\n\"\n",
    "    header += f\"Generated on: {timestamp}\\n\"\n",
    "    header += \"Seed\\tAccuracy\\tPrecision\\tRecall\\tF1\\tSpecificity\\tSensitivity\\tAUC\\n\"\n",
    "    \n",
    "    # Open file and write header\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(header)\n",
    "    \n",
    "    # Run for each seed\n",
    "    for seed in range(num_seeds):\n",
    "        # Run CV with current seed\n",
    "        metrics = random_forest_cv_performance(X, y, n_estimators=n_estimators, random_state=seed)\n",
    "        \n",
    "        # Calculate mean for each metric\n",
    "        mean_metrics = {\n",
    "            'accuracy': np.mean(metrics['accuracy']),\n",
    "            'precision': np.mean(metrics['precision']),\n",
    "            'recall': np.mean(metrics['recall']),\n",
    "            'f1': np.mean(metrics['f1']),\n",
    "            'specificity': np.mean(metrics['specificity']),\n",
    "            'sensitivity': np.mean(metrics['sensitivity']),\n",
    "            'auc': np.mean(metrics['auc'])\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        all_results['seed'].append(seed)\n",
    "        for metric in mean_metrics:\n",
    "            all_results[metric].append(mean_metrics[metric])\n",
    "        \n",
    "        # Write to file\n",
    "        with open(output_file, 'a') as f:\n",
    "            f.write(f\"{seed}\\t{mean_metrics['accuracy']:.4f}\\t{mean_metrics['precision']:.4f}\\t\"\n",
    "                    f\"{mean_metrics['recall']:.4f}\\t{mean_metrics['f1']:.4f}\\t\"\n",
    "                    f\"{mean_metrics['specificity']:.4f}\\t{mean_metrics['sensitivity']:.4f}\\t\"\n",
    "                    f\"{mean_metrics['auc']:.4f}\\n\")\n",
    "    \n",
    "    # Calculate final averages across all seeds\n",
    "    final_avg = {\n",
    "        'accuracy': np.mean(all_results['accuracy']),\n",
    "        'precision': np.mean(all_results['precision']),\n",
    "        'recall': np.mean(all_results['recall']),\n",
    "        'f1': np.mean(all_results['f1']),\n",
    "        'specificity': np.mean(all_results['specificity']),\n",
    "        'sensitivity': np.mean(all_results['sensitivity']),\n",
    "        'auc': np.mean(all_results['auc'])\n",
    "    }\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    final_std = {\n",
    "        'accuracy': np.std(all_results['accuracy']),\n",
    "        'precision': np.std(all_results['precision']),\n",
    "        'recall': np.std(all_results['recall']),\n",
    "        'f1': np.std(all_results['f1']),\n",
    "        'specificity': np.std(all_results['specificity']),\n",
    "        'sensitivity': np.std(all_results['sensitivity']),\n",
    "        'auc': np.std(all_results['auc'])\n",
    "    }\n",
    "    \n",
    "    # Append final averages to file\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(\"\\nFinal Average Performance Across All Seeds:\\n\")\n",
    "        f.write(\"----------------------------------------\\n\")\n",
    "        f.write(f\"Accuracy:    {final_avg['accuracy']:.4f}  {final_std['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Precision:   {final_avg['precision']:.4f}  {final_std['precision']:.4f}\\n\")\n",
    "        f.write(f\"Recall:      {final_avg['recall']:.4f}  {final_std['recall']:.4f}\\n\")\n",
    "        f.write(f\"F1 Score:    {final_avg['f1']:.4f}  {final_std['f1']:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {final_avg['specificity']:.4f}  {final_std['specificity']:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity: {final_avg['sensitivity']:.4f}  {final_std['sensitivity']:.4f}\\n\")\n",
    "        f.write(f\"AUC:         {final_avg['auc']:.4f}  {final_std['auc']:.4f}\\n\")\n",
    "        f.write(\"----------------------------------------\\n\")\n",
    "    \n",
    "    # Print final results to console\n",
    "    print(\"\\nFinal Average Performance Across All Seeds:\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"Accuracy:    {final_avg['accuracy']:.4f}  {final_std['accuracy']:.4f}\")\n",
    "    print(f\"Precision:   {final_avg['precision']:.4f}  {final_std['precision']:.4f}\")\n",
    "    print(f\"Recall:      {final_avg['recall']:.4f}  {final_std['recall']:.4f}\")\n",
    "    print(f\"F1 Score:    {final_avg['f1']:.4f}  {final_std['f1']:.4f}\")\n",
    "    print(f\"Specificity: {final_avg['specificity']:.4f}  {final_std['specificity']:.4f}\")\n",
    "    print(f\"Sensitivity: {final_avg['sensitivity']:.4f}  {final_std['sensitivity']:.4f}\")\n",
    "    print(f\"AUC:         {final_avg['auc']:.4f}  {final_std['auc']:.4f}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"\\nDetailed results saved to: {output_file}\")\n",
    "    \n",
    "    return final_avg, final_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36287e8c",
   "metadata": {},
   "source": [
    "#### BL Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3d139",
   "metadata": {},
   "source": [
    "##### BL using all BL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26738a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_avg, final_std = run_multiple_seeds(X_BLvsL[:, BL_features_to_analyse], y_BLvsL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f82490",
   "metadata": {},
   "source": [
    "##### BL using all persisent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec99fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_avg, final_std = run_multiple_seeds(X_BLvsL[:, WL_persist_features], \n",
    "                                          y_BLvsL,output_file=\"BL_WL-persistF_rf_100seed_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70dfe0",
   "metadata": {},
   "source": [
    "#### PWL Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e887f1",
   "metadata": {},
   "source": [
    "##### PWL using all PWL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_avg, final_std = run_multiple_seeds(X_PWLvsL[:,PWL_features_to_analyse], y_PWLvsL\n",
    "                                          ,output_file=\"PWL_allF_rf_100seed_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392528c",
   "metadata": {},
   "source": [
    "##### PWL using all persisent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_avg, final_std = run_multiple_seeds(X_PWLvsL[:,WL_persist_features], y_PWLvsL\n",
    "                                          ,output_file=\"PWL_WL-persistF_rf_100seed_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86448bda",
   "metadata": {},
   "source": [
    "# Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccefad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PWL_features_to_analyse=PWL_features_to_analyse.tolist()\n",
    "BL_features_to_analyse=BL_features_to_analyse.tolist()\n",
    "PWL_features_to_analyse_names=(np.array(feature_id_ls['OB_PWL'])[PWL_features_to_analyse]).tolist()\n",
    "BL_features_to_analyse_names=(np.array(feature_id_ls['OB_BL'])[BL_features_to_analyse]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# First, create dictionaries to map features to their names\n",
    "bl_feature_dict = dict(zip(BL_features_to_analyse, BL_features_to_analyse_names))\n",
    "pwl_feature_dict = dict(zip(PWL_features_to_analyse, PWL_features_to_analyse_names))\n",
    "\n",
    "# Get all unique features\n",
    "all_features = set(bl_feature_dict.keys()).union(set(pwl_feature_dict.keys()))\n",
    "\n",
    "# Categorize features\n",
    "wl_responding = []    # BL only (orange)\n",
    "wl_persistent = []    # Both BL and PWL (green)\n",
    "weight_remodelling = []  # PWL only (blue)\n",
    "\n",
    "for feat in all_features:\n",
    "    in_bl = feat in bl_feature_dict\n",
    "    in_pwl = feat in pwl_feature_dict\n",
    "    \n",
    "    if in_bl and in_pwl:\n",
    "        wl_persistent.append((feat, bl_feature_dict[feat]))  # Using BL name\n",
    "    elif in_bl:\n",
    "        wl_responding.append((feat, bl_feature_dict[feat]))\n",
    "    else:\n",
    "        weight_remodelling.append((feat, pwl_feature_dict[feat]))\n",
    "\n",
    "# Combine and sort features by category\n",
    "sorted_features = wl_responding + wl_persistent + weight_remodelling\n",
    "features_for_corr = [x[0] for x in sorted_features]\n",
    "features_for_corr_names = [x[1] for x in sorted_features]\n",
    "\n",
    "# Create color mapping for x-axis labels\n",
    "label_colors = []\n",
    "for feat, name in sorted_features:\n",
    "    if feat in bl_feature_dict and feat in pwl_feature_dict:\n",
    "        label_colors.append('#2CA02C')  # Green - WL-Persistent\n",
    "    elif feat in bl_feature_dict:\n",
    "        label_colors.append('#FF7F0E')  # Orange - WL-responding\n",
    "    else:\n",
    "        label_colors.append('#1F77B4')  # Blue - Weight Remodelling\n",
    "\n",
    "# Rest of your code remains the same...\n",
    "# Create individual DataFrames with sorted features\n",
    "lean_df = pd.DataFrame(feature_array['LEAN'][:, features_for_corr], columns=features_for_corr_names)\n",
    "ob_bl_df = pd.DataFrame(feature_array['OB_BL'][:, features_for_corr], columns=features_for_corr_names)\n",
    "ob_pwl_df = pd.DataFrame(feature_array['OB_PWL'][:, features_for_corr], columns=features_for_corr_names)\n",
    "\n",
    "# Convert to numeric and clean data\n",
    "lean_df = lean_df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "ob_bl_df = ob_bl_df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "ob_pwl_df = ob_pwl_df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "# Calculate correlation matrices AND p-values\n",
    "def calculate_corr_with_pvals(df):\n",
    "    corr_matrix = df.corr()\n",
    "    pval_matrix = np.zeros_like(corr_matrix)\n",
    "    \n",
    "    for i in range(len(features_for_corr_names)):\n",
    "        for j in range(len(features_for_corr_names)):\n",
    "            if i != j:\n",
    "                r, p = pearsonr(df.iloc[:, i], df.iloc[:, j])\n",
    "                pval_matrix[i, j] = p\n",
    "            else:\n",
    "                pval_matrix[i, j] = 1  # Diagonal\n",
    "    \n",
    "    return corr_matrix, pd.DataFrame(pval_matrix, index=corr_matrix.index, columns=corr_matrix.columns)\n",
    "\n",
    "corr_lean, p_lean = calculate_corr_with_pvals(lean_df)\n",
    "corr_ob_bl, p_ob_bl = calculate_corr_with_pvals(ob_bl_df)\n",
    "corr_ob_pwl, p_ob_pwl = calculate_corr_with_pvals(ob_pwl_df)\n",
    "\n",
    "# --- Plotting modifications ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 8), gridspec_kw={'width_ratios': [1, 1, 1]})\n",
    "\n",
    "def plot_heatmap_with_significance(ax, corr, pvals, title, show_yticks=True):\n",
    "    # Base heatmap\n",
    "    sns.heatmap(corr, ax=ax, cmap='coolwarm', center=0, annot_kws={\"va\": 'bottom'},\n",
    "                annot=True, fmt=\".2f\", vmin=0, vmax=1,\n",
    "                xticklabels=features_for_corr_names,\n",
    "                yticklabels=features_for_corr_names if show_yticks else False,\n",
    "                cbar=False)\n",
    "    \n",
    "    # Add significance asterisks inside the grid cells\n",
    "    for i in range(len(features_for_corr_names)):\n",
    "        for j in range(len(features_for_corr_names)):\n",
    "            if i != j:\n",
    "                p = pvals.iloc[i, j]\n",
    "                if p < 0.001:\n",
    "                    ax.text(j+0.5, i+0.7, '***', ha='center', va='center', \n",
    "                           fontsize=10, color='white' if abs(corr.iloc[i,j]) > 0.5 else 'black')\n",
    "                elif p < 0.01:\n",
    "                    ax.text(j+0.5, i+0.7, '**', ha='center', va='center', \n",
    "                           fontsize=10, color='white' if abs(corr.iloc[i,j]) > 0.5 else 'black')\n",
    "                elif p < 0.05:\n",
    "                    ax.text(j+0.5, i+0.7, '*', ha='center', va='center', \n",
    "                           fontsize=10, color='white' if abs(corr.iloc[i,j]) > 0.5 else 'black')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "\n",
    "# Plot all three heatmaps\n",
    "plot_heatmap_with_significance(axes[0], corr_lean, p_lean, 'LEAN', show_yticks=True)\n",
    "plot_heatmap_with_significance(axes[1], corr_ob_bl, p_ob_bl, 'OB_BL', show_yticks=False)\n",
    "plot_heatmap_with_significance(axes[2], corr_ob_pwl, p_ob_pwl, 'OB_PWL', show_yticks=False)\n",
    "\n",
    "# --- Add colorbar at the bottom ---\n",
    "cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.03])  # [left, bottom, width, height]\n",
    "fig.colorbar(axes[0].collections[0], cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "# --- Add legend on the right side ---\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='BL only',\n",
    "               markerfacecolor='#FF7F0E', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='BL+PWL',\n",
    "               markerfacecolor='#2CA02C', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='PWL only',\n",
    "               markerfacecolor='#1F77B4', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='*', color='k', label='p < 0.05',\n",
    "               markersize=10, linestyle='None'),\n",
    "    plt.Line2D([0], [0], marker='*', color='k', label='p < 0.01',\n",
    "               markersize=13, linestyle='None'),\n",
    "    plt.Line2D([0], [0], marker='*', color='k', label='p < 0.001',\n",
    "               markersize=16, linestyle='None')\n",
    "]\n",
    "\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1,0.7), fontsize=10)\n",
    "\n",
    "# --- Color labels and formatting ---\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    for label, color in zip(ax.get_xticklabels(), label_colors):\n",
    "        label.set_color(color)\n",
    "        label.set_fontweight('bold')\n",
    "    \n",
    "    if ax == axes[0]:\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "        for label, color in zip(ax.get_yticklabels(), label_colors):\n",
    "            label.set_color(color)\n",
    "            label.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.08, 0.9, 1])  # Adjusted for bottom colorbar\n",
    "\n",
    "plt.savefig('feature_correlations_sorted.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c500c",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9cf644e-1089-43f3-8e6a-95379091bcb4",
   "metadata": {},
   "source": [
    "def calculate_group_distances(X_group1, X_group2):\n",
    "    \"\"\"Calculate pairwise distances between two groups\"\"\"\n",
    "    distances = pairwise_distances(X_group1, X_group2, metric='euclidean')\n",
    "    return distances.flatten()\n",
    "\n",
    "def plot_combined_umaps(X_lean, X_ob_bl, X_ob_pwl, X_lean_respond, X_ob_pwl_respond, X_lean_persist, X_ob_pwl_persist, pwl_features_idx):\n",
    "    \"\"\"Plot combined UMAP visualizations in 4 subplots with shared legend\"\"\"\n",
    "    # Create the figure and subplots\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "    \n",
    "    # Create a list to store all embeddings for potential return\n",
    "    embeddings = []\n",
    "    \n",
    "    # Plot 1: LEAN vs OB_BL (all baseline features) - stays the same\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    X_combined = np.vstack([X_lean, X_ob_bl])\n",
    "    embedding = reducer.fit_transform(X_combined)\n",
    "    embeddings.append(embedding)\n",
    "    \n",
    "    scatter1 = axes[0].scatter(embedding[:len(X_lean), 0], embedding[:len(X_lean), 1], \n",
    "                              c='green', label='LEAN', s=50)\n",
    "    scatter2 = axes[0].scatter(embedding[len(X_lean):, 0], embedding[len(X_lean):, 1], \n",
    "                              c='red', label='OB_BL', s=50)\n",
    "    axes[0].set_title('UMAP: LEAN vs OB_BL\\n(All Baseline Features)')\n",
    "    \n",
    "    # Plot 2: LEAN vs OB_PWL (all PWL features)\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    X_lean_pwl = X_lean[:, pwl_features_idx]\n",
    "    X_ob_pwl_all = X_ob_pwl[:, pwl_features_idx]\n",
    "    X_combined_pwl = np.vstack([X_lean_pwl, X_ob_pwl_all])\n",
    "    embedding_pwl = reducer.fit_transform(X_combined_pwl)\n",
    "    embeddings.append(embedding_pwl)\n",
    "    \n",
    "    axes[1].scatter(embedding_pwl[:len(X_lean_pwl), 0], embedding_pwl[:len(X_lean_pwl), 1], \n",
    "                   c='green', s=50)\n",
    "    axes[1].scatter(embedding_pwl[len(X_lean_pwl):, 0], embedding_pwl[len(X_lean_pwl):, 1], \n",
    "                   c='blue', label='OB_PWL (all)', s=50)\n",
    "    axes[1].set_title('UMAP: LEAN vs OB_PWL\\n(All PWL Features)')\n",
    "    \n",
    "    # Plot 3: LEAN vs OB_PWL (responding features)\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    X_combined_respond = np.vstack([X_lean_respond, X_ob_pwl_respond])\n",
    "    embedding_respond = reducer.fit_transform(X_combined_respond)\n",
    "    embeddings.append(embedding_respond)\n",
    "    \n",
    "    axes[2].scatter(embedding_respond[:len(X_lean_respond), 0], embedding_respond[:len(X_lean_respond), 1], \n",
    "                   c='green', s=50)\n",
    "    axes[2].scatter(embedding_respond[len(X_lean_respond):, 0], embedding_respond[len(X_lean_respond):, 1], \n",
    "                   c='orange', label='OB_PWL (respond)', s=50)\n",
    "    axes[2].set_title('UMAP: LEAN vs OB_PWL\\n(WL Responding Features)')\n",
    "    \n",
    "    # Plot 4: LEAN vs OB_PWL (persistent features)\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    X_combined_persist = np.vstack([X_lean_persist, X_ob_pwl_persist])\n",
    "    embedding_persist = reducer.fit_transform(X_combined_persist)\n",
    "    embeddings.append(embedding_persist)\n",
    "    \n",
    "    axes[3].scatter(embedding_persist[:len(X_lean_persist), 0], embedding_persist[:len(X_lean_persist), 1], \n",
    "                   c='green', s=50)\n",
    "    axes[3].scatter(embedding_persist[len(X_lean_persist):, 0], embedding_persist[len(X_lean_persist):, 1], \n",
    "                   c='pink', label='OB_PWL (persist)', s=50)\n",
    "    axes[3].set_title('UMAP: LEAN vs OB_PWL\\n(WL Persistent Features)')\n",
    "    \n",
    "    # Add a single legend for all subplots\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    # Add the other labels that only appear in other subplots\n",
    "    handles.extend([\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='pink', markersize=10)\n",
    "    ])\n",
    "    labels.extend(['OB_PWL (all)', 'OB_PWL (respond)', 'OB_PWL (persist)'])\n",
    "    \n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "#    plt.savefig('Umap.pdf', \n",
    "#                format='pdf', \n",
    "#                bbox_inches='tight', \n",
    "#                dpi=300, \n",
    "#                pad_inches=0.1)           \n",
    "    plt.show()\n",
    "    return embeddings\n",
    "\n",
    "# Prepare the data\n",
    "wl_respond_idx = [BL_features_to_analyse.tolist().index(f) for f in WL_respond_features]\n",
    "wl_persist_idx = [BL_features_to_analyse.tolist().index(f) for f in WL_persist_features]\n",
    "pwl_features_idx = [PWL_features_to_analyse.tolist().index(f) for f in PWL_features_to_analyse]\n",
    "\n",
    "X_lean = feature_array['LEAN'][:, :len(BL_features_to_analyse)]\n",
    "X_ob_bl = feature_array['OB_BL'][:, :len(BL_features_to_analyse)]\n",
    "X_ob_pwl = feature_array['OB_PWL'][:, :len(BL_features_to_analyse)]\n",
    "\n",
    "X_lean_respond = X_lean[:, wl_respond_idx]\n",
    "X_ob_pwl_respond = X_ob_pwl[:, wl_respond_idx]\n",
    "X_lean_persist = X_lean[:, wl_persist_idx]\n",
    "X_ob_pwl_persist = X_ob_pwl[:, wl_persist_idx]\n",
    "\n",
    "# Generate the UMAP plots\n",
    "embeddings = plot_combined_umaps(X_lean, X_ob_bl, X_ob_pwl,\n",
    "                                X_lean_respond, X_ob_pwl_respond,\n",
    "                                X_lean_persist, X_ob_pwl_persist,\n",
    "                                pwl_features_idx)\n",
    "\n",
    "# Quantitative comparisons (updated to include all PWL features)\n",
    "dist_lean_ob_bl = calculate_group_distances(X_lean, X_ob_bl)\n",
    "X_lean_pwl = X_lean[:, pwl_features_idx]\n",
    "X_ob_pwl_all = X_ob_pwl[:, pwl_features_idx]\n",
    "dist_lean_ob_pwl_all = calculate_group_distances(X_lean_pwl, X_ob_pwl_all)\n",
    "dist_lean_ob_pwl_respond = calculate_group_distances(X_lean_respond, X_ob_pwl_respond)\n",
    "dist_lean_ob_pwl_persist = calculate_group_distances(X_lean_persist, X_ob_pwl_persist)\n",
    "\n",
    "# Perform all tests first\n",
    "_, pval_all_pwl = mannwhitneyu(dist_lean_ob_bl, dist_lean_ob_pwl_all, alternative='two-sided')\n",
    "_, pval_respond = mannwhitneyu(dist_lean_ob_bl, dist_lean_ob_pwl_respond, alternative='two-sided')\n",
    "_, pval_persist = mannwhitneyu(dist_lean_ob_bl, dist_lean_ob_pwl_persist, alternative='two-sided')\n",
    "_, pval_resp_vs_persist = mannwhitneyu(dist_lean_ob_pwl_respond, dist_lean_ob_pwl_persist, alternative='two-sided')\n",
    "\n",
    "# Prepare for p-value adjustment\n",
    "p_values = [pval_all_pwl, pval_respond, pval_persist, pval_resp_vs_persist]\n",
    "test_names = [\n",
    "    \"All PWL features vs baseline\",\n",
    "    \"Respond features vs baseline\",\n",
    "    \"Persist features vs baseline\",\n",
    "    \"Respond vs persist features\"\n",
    "]\n",
    "\n",
    "# Apply Benjamini-Hochberg correction\n",
    "reject, adj_pvals, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "\n",
    "# Create a dictionary for easier access\n",
    "adjusted_results = dict(zip(test_names, zip(p_values, adj_pvals, reject)))\n",
    "\n",
    "# Calculate effect sizes\n",
    "median_bl = np.median(dist_lean_ob_bl)\n",
    "improvement_all_pwl = median_bl - np.median(dist_lean_ob_pwl_all)\n",
    "improvement_respond = median_bl - np.median(dist_lean_ob_pwl_respond)\n",
    "improvement_persist = median_bl - np.median(dist_lean_ob_pwl_persist)\n",
    "\n",
    "# Updated results reporting with adjusted p-values\n",
    "print(\"Quantitative Results (with FDR-adjusted p-values):\")\n",
    "for test_name, (pval, adj_p, is_rej) in adjusted_results.items():\n",
    "    asterisk = \"**\" if is_rej else \"\"\n",
    "    print(f\"{test_name}:\")\n",
    "    print(f\"  Raw p-value = {pval:.4f}\")\n",
    "    print(f\"  Adjusted p-value = {adj_p:.4f}{asterisk}\")\n",
    "    if is_rej:\n",
    "        print(\"  Significant after multiple testing correction\")\n",
    "\n",
    "print(\"\\nMedian distances comparison:\")\n",
    "print(f\"LEAN-OB_BL (all features): {median_bl:.2f}\")\n",
    "print(f\"LEAN-OB_PWL (all PWL features): {np.median(dist_lean_ob_pwl_all):.2f}\")\n",
    "print(f\"LEAN-OB_PWL (respond features): {np.median(dist_lean_ob_pwl_respond):.2f}\")\n",
    "print(f\"LEAN-OB_PWL (persist features): {np.median(dist_lean_ob_pwl_persist):.2f}\")\n",
    "\n",
    "print(\"\\nImprovement in similarity to LEAN:\")\n",
    "print(f\"Using all PWL features: {improvement_all_pwl:.2f} ({improvement_all_pwl/median_bl*100:.1f}%)\")\n",
    "print(f\"Using respond features: {improvement_respond:.2f} ({improvement_respond/median_bl*100:.1f}%)\")\n",
    "print(f\"Using persist features: {improvement_persist:.2f} ({improvement_persist/median_bl*100:.1f}%)\")\n",
    "\n",
    "# Specific interpretation for the key comparison\n",
    "resp_vs_persist_adj_p = adjusted_results[\"Respond vs persist features\"][1]\n",
    "if resp_vs_persist_adj_p < 0.05:\n",
    "    if np.median(dist_lean_ob_pwl_respond) < np.median(dist_lean_ob_pwl_persist):\n",
    "        print(\"\\nRespond features show significantly greater similarity to LEAN than persistent features (after adjustment)\")\n",
    "    else:\n",
    "        print(\"\\nPersistent features show significantly greater similarity to LEAN than respond features (after adjustment)\")\n",
    "else:\n",
    "    print(\"\\nNo significant difference between respond and persistent features in similarity to LEAN (after adjustment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb5c55",
   "metadata": {},
   "source": [
    "# Test Feature Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comparisons(feature_array, WL_respond_or_persist_features, group1, group2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate statistical test results (parametric or non-parametric) for all specified features between two groups\n",
    "    \n",
    "    Args:\n",
    "        feature_array: Dictionary of numpy arrays (each [samples  features])\n",
    "        WL_respond_features: List of feature indices to analyze\n",
    "        group1, group2: Group names to compare\n",
    "        alpha: Significance level for normality and variance tests\n",
    "        \n",
    "    Returns: DataFrame with feature names, test type, p-values, effect sizes, adjusted p-values, and log fold changes\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for feature in WL_respond_or_persist_features:\n",
    "        try:\n",
    "            # Extract data for both groups - ensure they're numeric\n",
    "            data1 = np.asarray(feature_array[group1][:, feature], dtype=float)\n",
    "            data2 = np.asarray(feature_array[group2][:, feature], dtype=float)\n",
    "            \n",
    "            # Remove any potential NaN/inf values\n",
    "            data1 = data1[np.isfinite(data1)]\n",
    "            data2 = data2[np.isfinite(data2)]\n",
    "            \n",
    "            # Skip if either group has no valid data\n",
    "            if len(data1) < 3 or len(data2) < 3:  # Need at least 3 samples for normality test\n",
    "                continue\n",
    "                \n",
    "            # Calculate log fold change (base 2)\n",
    "            mean1 = np.mean(data1)\n",
    "            mean2 = np.mean(data2)\n",
    "            # Add small pseudocount to avoid log(0)\n",
    "            #log_fc = np.log2((mean1 + 1e-5) / (mean2 + 1e-5))\n",
    "            log_fc=(mean1 + 1e-5) / (mean2 + 1e-5)\n",
    "            \n",
    "            # Check normality (Shapiro-Wilk test)\n",
    "            _, p1 = shapiro(data1)\n",
    "            _, p2 = shapiro(data2)\n",
    "            \n",
    "            # Check variance homogeneity (Levene's test)\n",
    "            _, p_levene = levene(data1, data2)\n",
    "            \n",
    "            # Decide which test to use\n",
    "            if p1 > alpha and p2 > alpha and p_levene > alpha:\n",
    "                # Parametric (t-test)\n",
    "                test_type = 't-test'\n",
    "                stat, p = ttest_ind(data1, data2, equal_var=True)\n",
    "                \n",
    "                # Calculate Cohen's d effect size\n",
    "                n1, n2 = len(data1), len(data2)\n",
    "                pooled_std = np.sqrt(((n1-1)*np.std(data1, ddof=1)**2 + (n2-1)*np.std(data2, ddof=1)**2) / (n1 + n2 - 2))\n",
    "                effect_size = (np.mean(data1) - np.mean(data2)) / pooled_std\n",
    "            else:\n",
    "                # Non-parametric (Mann-Whitney U)\n",
    "                test_type = 'Mann-Whitney U'\n",
    "                stat, p = mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "                \n",
    "                # Calculate rank-biserial correlation effect size\n",
    "                n1, n2 = len(data1), len(data2)\n",
    "                effect_size = 1 - (2 * stat) / (n1 * n2)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'test_type': test_type,\n",
    "                'p_value': p,\n",
    "                'effect_size': effect_size,\n",
    "                'log2_fold_change': log_fc,\n",
    "                'mean_group1': mean1,\n",
    "                'mean_group2': mean2,\n",
    "                'comparison': f\"{group1}_vs_{group2}\"\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature {feature}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Adjust p-values for multiple comparisons (skip NaN values)\n",
    "    pvals = df['p_value'].values\n",
    "    if len(pvals) > 0 and np.any(~np.isnan(pvals)):\n",
    "        df['adjusted_p'] = multipletests(pvals, method='fdr_bh')[1]\n",
    "    else:\n",
    "        df['adjusted_p'] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/miRNA/Acute_WL/Log2_MachineLearning_Results')\n",
    "\n",
    "all_features=list(set(BL_features_to_analyse + PWL_features_to_analyse))\n",
    "bl_vs_lean = calculate_comparisons(feature_array, all_features, 'OB_BL', 'LEAN')\n",
    "pwl_vs_lean = calculate_comparisons(feature_array, all_features, 'OB_PWL', 'LEAN')\n",
    "\n",
    "if not bl_vs_lean.empty and not pwl_vs_lean.empty:\n",
    "    DE_sig_df = pd.merge(bl_vs_lean, pwl_vs_lean, on='feature', \n",
    "                         suffixes=('_BL_vs_Lean', '_PWL_vs_Lean'))\n",
    "    \n",
    "    # Calculate changes\n",
    "    DE_sig_df['p_value_change'] = DE_sig_df['adjusted_p_PWL_vs_Lean'] - DE_sig_df['adjusted_p_BL_vs_Lean']\n",
    "    DE_sig_df['effect_size_change'] = DE_sig_df['effect_size_PWL_vs_Lean'] - DE_sig_df['effect_size_BL_vs_Lean']\n",
    "    DE_sig_df['abs_effect_size_change'] = abs(DE_sig_df['effect_size_change'])\n",
    "    DE_sig_df['log2_fold_change_change'] = DE_sig_df['log2_fold_change_PWL_vs_Lean'] - DE_sig_df['log2_fold_change_BL_vs_Lean']\n",
    "    DE_sig_df['abs_log2_fold_change_change'] = abs(DE_sig_df['log2_fold_change_change'])\n",
    "    \n",
    "    # Sort by largest changes (you can choose either effect size or fold change)\n",
    "    DE_sig_df = DE_sig_df.sort_values(\n",
    "        by='abs_log2_fold_change_change', \n",
    "        ascending=False)\n",
    "\n",
    "# Create a mapping from feature_id to feature_name\n",
    "id_to_name = pd.concat([df_WL_respond_features, df_WL_persist_features, df_WL_remodel_features], axis=0).reset_index(drop=True).set_index('feature_id')['feature_name']\n",
    "# Map the 'feature' column to corresponding feature names\n",
    "DE_sig_df.insert( 0,  'feature_name', DE_sig_df['feature'].map(id_to_name))\n",
    "#DE_sig_df.to_csv('DE_sig_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ac447-3038-470b-9c7e-08f1d5a330bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_sig_df[['feature_name','log2_fold_change_BL_vs_Lean','log2_fold_change_PWL_vs_Lean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_table = {\n",
    "    \"Appear in BL classi\": [\"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\"],\n",
    "    \"Appear in PWL classi\": [\"Yes\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"],\n",
    "    \"Type\": [\"Remodelling\", \"Persistent\", \"Respond\", \"Respond\", \"Respond\", \n",
    "             \"Persistent\", \"Remodelling\", \"Persistent\", \"Persistent\"],\n",
    "    \"feature_name\": [\n",
    "        \"hsa-miR-630\", \"hsa-miR-320e\", \"hsa-miR-4454+hsa-miR-7975\", \"hsa-miR-574-5p\", \"hsa-miR-378i\",\n",
    "        \"hsa-miR-127-3p\", \"hsa-miR-370-3p\", \"hsa-miR-151a-3p\", \"hsa-miR-30a-5p\"\n",
    "    ],\n",
    "    \"adjusted_p_BL_vs_Lean\": [0.005746, 0.001646, 0.001991, 0.001646, 0.002478, 0.003133, 0.028172, 0.001646, 0.001646],\n",
    "    \"log2_fold_change_BL_vs_Lean\": [1.449708, 1.4342, 3.007923, 1.053653, 1.088234, 2.727928, 2.260897, 2.418573, 1.200762],\n",
    "    \"Up/Down in Obesity (BL)\": [\"Up\", \"Up\", \"Up\", \"Up\", \"Up\", \"Up\", \"Up\", \"Up\", \"Up\"],\n",
    "    \"adjusted_p_PWL_vs_Lean\": [0.007662, 0.001765, 0.013526, 0.014138, 0.013105, 0.003319, 0.003708, 0.007662, 0.003319],\n",
    "    \"log2_fold_change_PWL_vs_Lean\": [3.766372, 2.280259, 3.370968, 0.739299, 0.795522, 2.582456, 2.317412, 2.36984, 1.206501],\n",
    "    \"Up/Down in Obesity (PWL)\": [\"Up\", \"Up\", \"Up\", \"NS\", \"NS\", \"Up\", \"Up\", \"Up\", \"Up\"]\n",
    "}\n",
    "\n",
    "DE_table = pd.DataFrame(DE_table)\n",
    "DE_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8198c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 6))  # Slightly wider figure\n",
    "\n",
    "# Color and marker settings\n",
    "style_map = {\n",
    "    'Respond': {'color': '#FF7F0E', 'marker': 's', 'label': 'BL only'},\n",
    "    'Remodelling': {'color': '#1F77B4', 'marker': '^', 'label': 'PWL only'},\n",
    "    'Persistent': {'color': '#2CA02C', 'marker': 'o', 'label': 'BL+PWL'}\n",
    "}\n",
    "\n",
    "# Plot each miRNA\n",
    "for idx, row in DE_table.iterrows():\n",
    "    category = row['Type']\n",
    "    style = style_map[category]\n",
    "    \n",
    "    # Plot BL and PWL points with connecting line\n",
    "    plt.plot([1, 2], \n",
    "             [row['log2_fold_change_BL_vs_Lean'], row['log2_fold_change_PWL_vs_Lean']], \n",
    "             color=style['color'], \n",
    "             alpha=0.3, \n",
    "             linestyle='--')\n",
    "    \n",
    "    # Plot BL point (for all categories now, including Remodelling)\n",
    "    plt.scatter(1, row['log2_fold_change_BL_vs_Lean'], \n",
    "                color=style['color'], \n",
    "                marker=style['marker'], \n",
    "                s=100,\n",
    "                edgecolor='white',\n",
    "                linewidth=1,\n",
    "                label=style['label'] if idx in [0,2,5] else \"\")\n",
    "    \n",
    "    # Plot PWL point for all\n",
    "    plt.scatter(2, row['log2_fold_change_PWL_vs_Lean'], \n",
    "                color=style['color'], \n",
    "                marker=style['marker'], \n",
    "                s=100,\n",
    "                edgecolor='white',\n",
    "                linewidth=1)\n",
    "    \n",
    "    # Annotate values with adjusted vertical and horizontal positions\n",
    "    y_offset = 0.08 if idx % 2 else -0.1  # Alternate offset direction\n",
    "    x_offset1 = -0.03 if idx % 3 == 0 else 0.03  # Slight horizontal offset for BL points\n",
    "    x_offset2 = -0.03 if idx % 3 == 1 else 0.03  # Slight horizontal offset for PWL points\n",
    "    \n",
    "    plt.text(1 + x_offset1, row['log2_fold_change_BL_vs_Lean'] + y_offset, \n",
    "             f\"{row['log2_fold_change_BL_vs_Lean']:.2f}\", \n",
    "             ha='center', va='center', fontsize=8)\n",
    "    plt.text(2 + x_offset2, row['log2_fold_change_PWL_vs_Lean'] + y_offset, \n",
    "             f\"{row['log2_fold_change_PWL_vs_Lean']:.2f}\", \n",
    "             ha='center', va='center', fontsize=8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xticks([1, 2], ['Baseline (OB_BL)', 'Post Weight Loss (OB_PWL)'], fontsize=10)\n",
    "plt.ylabel('log2 Fold Change', fontsize=10)\n",
    "plt.title('miRNA Expression Changes:\\nBaseline vs Post Weight Loss', fontsize=12, pad=15)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim([0.8, 2.2])  # Adjusted x-limits for better spacing\n",
    "\n",
    "plt.axhline(y=1, color='red', linestyle=':', alpha=0.5)\n",
    "\n",
    "# Add legend\n",
    "handles = []\n",
    "for category, style in style_map.items():\n",
    "    handles.append(plt.Line2D([0], [0], \n",
    "                         marker=style['marker'], \n",
    "                         color='w', \n",
    "                         markerfacecolor=style['color'],\n",
    "                         markersize=8, \n",
    "                         label=style['label']))\n",
    "plt.legend(handles=handles, loc='upper left', fontsize=9)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "#plt.savefig('DE_lineplot.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4880931",
   "metadata": {},
   "source": [
    "##### interpretation of Up/Down Regulation:\n",
    "<br> >1 : Up regulated in Group1 (Obese)\n",
    "<br><-1: Down regulated in Group1 (Obese)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6499ff",
   "metadata": {},
   "source": [
    "maybe not useful:\n",
    "<br>Except Feature 153, both pvalue change and effect size change are very small, means they are quite persisent to WL\n",
    "<br> For feature 153, it seems like after WL it normalized (with effect size = 0.18). Again this could be due to by itself it respond to WL, however, it is the interaction (co-expression) of it with other features make it persisent pattern to WL (This is proved in the tsne plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef8753",
   "metadata": {},
   "source": [
    "# miRNA gene Analysis tools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9cfcc5",
   "metadata": {},
   "source": [
    "##### find if a mirna-gene association is valid\n",
    "https://dianalab.e-ce.uth.gr/html/dianauniverse/index.php?r=microT_CDS/results&keywords=hsa-miR-30a-5p&genes=&mirnas=hsa-miR-30a-5p%20&descr=&threshold=0.7\n",
    "##### mirna-mRNA network and enrichment PPI analysis\n",
    "https://www.mirnet.ca/miRNet/home.xhtml\n",
    "https://mirdb.org/cgi-bin/search.cgi\n",
    "##### miRNA disease association databse\n",
    "http://www.cuilab.cn/hmdd\n",
    "##### finding associated genomic variation location\n",
    "https://www.ncbi.nlm.nih.gov/gene/442893\n",
    "##### GWAS catalogue\n",
    "https://www.ebi.ac.uk/gwas/variants/rs746839"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
